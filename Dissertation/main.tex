\documentclass[12pt]{scrartcl}

\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing

\title{Honors Thesis}
\author{Gina Chun}


\begin{document}

\maketitle

\newpage

\begin{abstract}
  Abstract...
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}\label{Introduction}



\subsection{Why Carbon is Important}\label{Test}

% In this section to say blah

Carbon is one of the most important and widely investigated
elements. Carbon is all around us in a variety of forms and is an area
of study which has huge significance across many scientific fields and
applications. Carbon is in living organisms and is essential for
living organisms, thus the study of carbon would further develop the
complex workings of the environment, ecosystems and the carbon
cycle. It would also help the understanding of fossil fuels, aid the
challenges of energy-related issues, and work towards solutions for
climate change. Carbon materials are also intertwined in modern
economic industries such as in steel, clothing, pigment, plastic and
synthetic materials, and in electrodes and other similar electricity
conducting machinery. Studying carbon allows for further advancement
in these important industries which would lead to better and more
easily accessible devices, tools, and products for general society.

Now referencing the introduction \ref{Test}

\subsection{Why Use Machine Learning}
As carbon is an important element, its versatility makes it a very
complex subject of study. Carbon manifests in various physical forms,
called allotropes, each with very different properties. Because of
this, studying carbon can be difficult with conventional
methods. Using Gaussian Approximation Potentials (GAP), machine
learning can be much more cost and time efficient than traditional
scientific methods of experimentation. Machine learning also opens up
a new space of empirical methods for experiments that scientists
cannot physically replicate with traditional techniques. For example,
machine learning can be used for experiments in very small spatial
scales or for large scale simulations all while maintaining high
performance accuracy.

Here is an example of a citation: \cite{gap20}, \cite{DGL}

\newpage

\section{Background}

\subsection{Chemistry}
\begin{itemize}
    \item[!] Machine Learning Meets Quantum Physics textbook
\end{itemize}

% Introduction to material modeling
Using machine learning to study any kind of material, not just carbon, 
is just one method under a wider category of techniques called material modeling. 
Material modeling in chemistry is the method of using computer simulations to predict 
physical and chemical properties of a material in replace of real-world experiments. 
This process of predicting the properties of a given material is called a \emph{forward 
problem}. Through existing tools such as statistical mechanics and the Schrödinger 
equation, which will be talked about later, an exact unique solution exists for the 
forward problem of predicting material properties and is the reason why many opt to 
use this procedure. Material modeling also uses this approach because the inverse 
problem, constructing a material through a set of pre-determined properties, 
is much more difficult and costly to do.

\subsubsection{Geometry and Structure}
% Structure property relation for molecules
``Structure determines properties'' is a very important and central concept in chemistry. 
This structure-property relationship explains that since all materials are made of atoms, identifying 
and describing a material comes down to simply understanding its atomic structure. For studying 
structures, there is never a need to look at the entire material as well. That would be too many 
atoms to study at once and atoms of most solid materials, like crystalline carbon, are arranged in 
an organized repetitive pattern so it is sufficient enough to look at just a patch of that pattern. 
Studying the properties of a microscopic structure, in general, translates well to its macroscopic 
structure which allows it to pair well with the material modeling approach.

% molecular \& material properties: Electronic properties (for hard degrees of freedom, crystalline)
% Born–Oppenheimer approximation, ground state, atomization energy
An atom can be described as having a heavy nucleus surrounded by a cloud of light 
electrons and since the mass of an atom is concentrated on its nucleus, the atom's 
position is associated with the position of the nucleus. Thus moving atoms can be described
as moving nuclei being followed by their clouds of electrons. The Born–Oppenheimer 
approximation is the mathematical formulation of this concept and is the foundation 
of material modeling. Molecular and material properties can be roughly divided into 
two categories: electronic and thermodynamic properties.Electronic properties are 
essentially the properties of that electron cloud that is following the nucleus and 
is best described through quantum mechanics, which will be covered in more detail later. 
To give a general overview, the electronic cloud can be in certain states such as its 
lowest-energy state called the \emph{ground state}. Some ground state properties include the
atomization energy, which is the energy released when a molecule is formed, and the 
dipole moment and polarizability, which describes the shape and responsiveness of the
cloud. Since thermodynamic properties can be more easily derived through knowing the electronic 
properties first, this paper and its method will be predicting ground-state electronic properties 
so that is what the background will be focused on. 


\subsubsection{Quantum Mechanics}
% object being in multiple states
% Schrödinger equation, DFT, PES
% computational cost with solving the equation
Quantum mechanics is the set of laws that describe objects on an atomic level. One of the core
principles is that an object can be in multiple states. An object like an electron can be in any 
combination of these two states. Mathematically, this can be expressed as a linear combination of 
two basis vectors that act as the two position states. The state of this object can be expressed 
as a \emph{wave function}, which generalizes the two position vectors to the infinite number of 
positions in the three-dimensional space. This fundamental idea is why electrons are commonly 
referred to as clouds.

The laws of quantum mechanics also states that each observable, quantifiable, physical property that 
an atomic object has is associated with a linear operator. The actual value of the quantity is given 
by the eignevalues of the operator. One of the most important operators is the energy operator, called 
the Hamiltonian, and its associated eigenvalue equation is called a Schrödinger equation. The energy of 
a particular eigenstate as a function of the nuclear positions is called a \emph{potential energy surface}
and it completely determines the dynamics of the motion of the atoms. The solution to this equation 
allows many electronic properties of both ground and excited states to be determined. The Schrödinger 
equation can be mathematically solved for singular atoms but it gets more difficult for more complex molecules. 
There are methods such as the density functional theory and the Hartree-Fock that try to 
solve this issue by manipulating the Hamiltonian or the vector space itself, but their computational 
costs make these methods practically inefficient. Once the Schrödinger equation is solved, evaluating 
properties such as atomization energy or dipole moment can be done through the eigenvalues and eigenstates.


\newpage


\subsection{Machine Learning}
\begin{itemize}
    \item[!] https://distill.pub/2021/gnn-intro/
\end{itemize}
\subsubsection{Graph Represented Data}
% graph representation (molecules as graphs)
% types of problems that use graph structured data 
% integrating graphs in neural networks (matrices, adjacency lists)
In a molecule, different pairs of atoms and bonds have different distances so it is easy to describe
these molecules as a graph. The atoms are the nodes and the bonds are the undirected edges. In general, 
there are three types of prediction tasks that are done on graphs: graph-level, node-level, and edge-level. 
Graph-level approaches predict a single property for a whole graph while a node-level approach predicts 
a property for each node in the graph. Edge-level approaches predict the property or existence of edges 
in a particular graph. This paper focuses on the graph-level task of predicting the energy of carbon 
molecules. 

Integrating these graphs into machine learning models will depend on how they are represented. 
An obvious representation method would be the adjacency matrix since it captures the graph's connectivity. 
However, the more nodes there are, the more space-inefficient the adjacency matrix becomes and in addition,
the adjacency matrix of a molecule is not unique. There are many different ways to express the same 
connectivity with these matrices which causes the model to be permutation invariant since it will see 
these matrices as different molecules when they are actually the same. A better memory-efficient representation 
is adjacency lists where each item in this list describes the connection between just two nodes. This cuts out 
a lot of the parts of the graph that are disconnected that would be needlessly recorded in an adjacency matrix.
% coulomb matrix?
% look at textbook for graphs as well

\subsubsection{Graph Neural Networks}    
\begin{itemize}
    \item formal definition of graph neural networks
    \item Graphic convolution
    \item pooling
    \item message passing
    \begin{itemize}
        \item https://dl.acm.org/doi/10.5555/3305381.3305512
        \item Machine Learning Meets Quantum Physics textbook Ch 10
    \end{itemize}
\end{itemize}

%\begin{figure}
  %\centering
  %\includegraphics[scale=2]{carbon}
  
  %\caption{Nice caption}\label{fig:my_figure}
%\end{figure}


%In figure \ref{fig:my_figure}


\newpage 

\section{Method}
\subsubsection{Carbon GAP 20 dataset (describe, why)}  
\subsubsection{graph model (describe, why)}  
Not permutation invariance yet?
\newline The model itself is semi permutation invariant but the local environments must be permutation invariant for the model to be COMPLETELY permutation invariant
\newline Also columb matrix creating rotation and translation invariance in paper
\newline What if everyone becomes the neighbors: How the local level is more important
 


\newpage

\section{Experimental Evaluation}
\subsubsection{other experiments}  
\begin{itemize}
    \item papers citing GAP 20
    \item Data-efficient iterative training of Gaussian approximation potentials: Application to surface structure determination of rutile IrO2 and RuO2 (https://aip.scitation.org/doi/abs/10.1063/5.0071249)
    \item A comprehensive assessment of empirical potentials for carbon materials (https://aip.scitation.org/doi/abs/10.1063/5.0052870)
    \item Transferability of machine learning potentials: Protonated water neural network potential applied to the protonated water hexamer (https://aip.scitation.org/doi/full/10.1063/5.0035438)
    \item Machine learning interatomic potential developed for molecular simulations on thermal properties of Beta-Ga2O3 (https://aip.scitation.org/doi/abs/10.1063/5.0027643)
\end{itemize}
\newpage

\section{Conclusion}

\newpage

\bibliographystyle{plain}
\bibliography{ref}


\end{document}
