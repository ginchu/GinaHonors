\documentclass[12pt, abstract = true]{scrartcl}

\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{cite}
\doublespacing

\title{Polymorphic Modelling the Potential Energy Surface of Carbon Systems using Graph Neural Networks}
\author{Gina Chun}


\begin{document}

\maketitle

\newpage

\begin{abstract}
% Abstract: 1) the overall purpose 2) the basic method 3) result of analysis 4) interpretations and conclusions

We present a machine learning method to model the potential energy surface of carbon structures with the following properties: polymorphic and physically-plausible. The model is polymorphic in that it models carbon structures regardless of the number of carbon atoms in the molecule. The model is physically-plausible because it is not dependent on the specific Cartesian coordinates of every single atom, making it invariant to translation, rotation, and permutation. 

The design of this method starts with graph representation for every molecule with the edges holding the distances between an atom's nearest neighbors so that only the local environment is captured. It also uses a message passing graph neural network (MPGNN) which allows nodes to learn the data from neighboring nodes. This approach comes from how molecules are composed of structural units that are approximately constant, thus representing a molecule as a network of local "neighborhoods" produces local features that can be aggregated into producing higher-level features of the entire molecule. 

The results show that the MPGNN is indeed learning the potential energy surface and is consistently improving as the number of message passing increases. A comparison between the MPGNN and a dummy regressor and an untrained MPGNN proves that the model is functioning as it should and is properly learning, but a comparison with the QUIP software shows that the MPGNN model still has room for improvement. The present work demonstrates a less costly approach to model the potential energy surface of carbon structures with a MPGNN, considering that current frameworks still have a high computational cost. The findings also show the potential for further improvement on the model and better future performance through even more fine tuning.
  
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}\label{Introduction}

\subsection{Why Carbon is Important}

Carbon is one of the most important and widely investigated
elements. Carbon is all around us in a variety of forms and is an area
of study which has huge significance across many scientific fields and
applications. The study of carbon would help the understanding of 
fossil fuels, aid the challenges of energy-related issues, and work towards solutions for
climate change. Carbon materials are also intertwined in modern industries and technologies 
such as in steel, clothing, synthetic materials, and even in the silicon microchips in smartphones. 
A better understanding of carbon will lead to advancement in technology and more innovation. 
This can in turn lead to general increase in the quality of living, not just in the US but also 
in other countries as well, which is important for people and communities in need for more easily accessible technology. 
Not only will these technologies be cheaper, their advancement will also make 
them more sustainable, eco-friendly, and ethical as well.

To better understand carbon, studying the energy of different carbon molecules is very important. 
Knowing the potential energy of a carbon structure will allow us to derive other important properties 
of that carbon molecule. Being able to easily predict the potential energy can help unlock much more 
information on carbon, but is currently difficult because of all the different configurations carbon 
can be in. Carbon has such vast structural diversity which makes it particularly arduous to study but 
also that more important to study as well.


\subsection{Why Use Machine Learning}
As carbon is an important element, its versatility makes it a very
complex subject of study. Carbon manifests in various physical forms,
called allotropes, each with very different properties. Because of
this, studying carbon can be difficult and very expensive with conventional
methods. Machine learning can be much more cost and time efficient than traditional
scientific methods of experimentation. It also opens up a new space of methods for 
experiments that scientists cannot physically replicate with traditional techniques. 
For example, machine learning can be used for experiments in very small spatial
scales or for large scale simulations all while maintaining high
performance accuracy. It also allows researchers to take advantage of similarities 
between certain physical systems and learning systems, such as neural networks, to be 
able to model these systems and to also just learn more about areas that are difficult 
to model otherwise such as quantum mechanics. Current theories and equations in quantum 
mechanics can calculate energy for a singular atom but it becomes increasingly difficult 
for complex molecules. Thus the application of machine learning and neural networks 
can greatly help the complex and costly study of carbon with all of its various geometries.

\subsection{Using Machine Learning on Carbon Molecules}
Currently, there are machine learning models that do predict the potential energy of carbon molecules. However, they are still very computationally expensive and require a large, comprehensive amount of data. We propose a lower cost machine learning approach to learning the potential energy of carbon molecules. Our method is to represent carbon molecules as graphs and have a graph neural network with message passing learn and predict on this graph-represented data. Our results show that this neural network is able to learn the potential energy of carbon structures and consistently improves as the number of message passing increases and other parameters are adjusted. Our model still needs improvement to match current chemistry softwares that predict potential energy but our experiments show the potential for even more promising performance and ways to improve our model for future research.
\newpage

\section{Background}

\subsection{Chemistry}

% Machine Learning Meets Quantum Physics textbook
% Introduction to material modeling
Using machine learning to study any kind of material, not just carbon, 
is just one method under a wider category of techniques called material modeling. 
Material modeling in chemistry is the method of using computer simulations to predict 
physical and chemical properties of a material in replace of real-world experiments. 
This process of predicting the properties of a given material is called a \emph{forward 
problem}. Through existing tools such as statistical mechanics and the Schrödinger 
equation, which will be talked about later, an exact unique solution exists for the 
forward problem of predicting material properties and is the reason why many opt to 
use this procedure. Material modeling also uses this approach because the inverse 
problem, constructing a material through a set of pre-determined properties, 
is much more difficult and costly to do.

\subsubsection{Geometry and Structure}
% Structure property relation for molecules
``Structure determines properties'' is a very important and central concept in chemistry. 
This structure-property relationship explains that since all materials are made of atoms, identifying 
and describing a material comes down to simply understanding its atomic structure. For studying 
structures, there is never a need to look at the entire material as well. That would be too many 
atoms to study at once and atoms of most solid materials, like crystalline carbon, are arranged in 
an organized repetitive pattern so it is sufficient enough to look at just a patch of that pattern. 
Studying the properties of a microscopic structure, in general, translates well to its macroscopic 
structure which allows it to pair well with the material modeling approach.


% molecular \& material properties: Electronic properties (for hard degrees of freedom, crystalline)
% Born–Oppenheimer approximation, ground state, atomization energy
An atom can be described as having a heavy nucleus surrounded by a cloud of light 
electrons and since the mass of an atom is concentrated on its nucleus, the atom's 
position is associated with the position of the nucleus. Thus moving atoms can be described
as moving nuclei being followed by their clouds of electrons. 



The Born–Oppenheimer approximation is the mathematical formulation of this concept and 
is the foundation of material modeling. Molecular and material properties can be 
roughly divided into two categories: electronic and thermodynamic properties. Electronic 
properties are essentially the properties of that electron cloud that is following the 
nucleus and is best described through quantum mechanics, which will be covered in more 
detail later. To give a general overview, the electronic cloud can be in certain states 
such as its lowest-energy state called the \emph{ground state}. Some ground state properties include the atomization energy, which is the energy released when a molecule is formed, and the 
dipole moment and polarizability, which describes the shape and responsiveness of the
cloud. Since thermodynamic properties can be more easily derived through knowing the electronic 
properties first, we will be predicting ground-state electronic properties 
so that is what the background will be focused on. 


\subsubsection{Quantum Mechanics}
% object being in multiple states
% Schrödinger equation, DFT, PES
% computational cost with solving the equation
Quantum mechanics is the set of laws that describe objects on an atomic level. One of the core
principles is that an object can be in multiple states. An object like an electron can be in any 
combination of these two states. Mathematically, this can be expressed as a linear combination of 
two basis vectors that act as the two position states. The state of this object can be expressed 
as a \emph{wave function}, which generalizes the two position vectors to the infinite number of 
positions in the three-dimensional space. This fundamental idea is why electrons are commonly 
referred to as clouds.

\begin{figure}
  \centering
  \includegraphics[scale=.4]{pes.png}
  \includegraphics[scale=.4]{pes2.png}
  
  \caption{Example of a 3-D and 2-D PES. The top graph shows the z-axis representing the potential energy, red being the highest and violet being the lowest potential energy. The lower graph has the distance between two atoms as the x-axis and potential energy on the y-axis. Image source from LibreTexts\cite{pesimage}}\label{fig:pes}
\end{figure}

The laws of quantum mechanics also states that each observable, quantifiable, physical property that 
an atomic object has is associated with a linear operator. The actual value of the quantity is given 
by the eignevalues of the operator. One of the most important operators is the energy operator, called 
the Hamiltonian, and its associated eigenvalue equation is called a Schrödinger equation. The energy of 
a particular eigenstate as a function of the nuclear positions is called a \emph{potential energy surface}
and it completely determines the dynamics of the motion of the atoms. The potential energy surface is shown visually in figure \ref{fig:pes}. The solution to this equation  allows many electronic properties of both 
ground and excited states to be determined. The Schrödinger equation can be mathematically solved for 
singular atoms but it gets more difficult for more complex molecules. There are methods such as the density 
functional theory and the Hartree-Fock that try to solve this issue by manipulating the Hamiltonian or the vector space itself, but their computational costs make these methods practically inefficient. Once the Schrödinger 
equation is solved, evaluating properties such as atomization energy or dipole moment can be done 
through the eigenvalues and eigenstates.

\subsubsection{QM Software} 

Currently, there are programs used in computational chemistry to implement the methods mentioned above to 
calculate properties. One software, called Vienna Ab initio Simulation Package or VASP, performs ab initio 
quantum mechnical calculations and uses DFT as its base methodology. Another software package, called 
QUantum Mechanics and Interatomic Potentials or QUIP\cite{quip}, has a collection of tools to conduct 
molecular dynamics simulations and works with other external packages such as LAMMPS or ASE. An example run may look like something in Figure \ref{fig:quip} where the QUIP command is executed on the command line using a referenced XYZ file. The XYZ file must in a format where the first line is the number of atoms and the second line is a series of key and value pairs which must have the Lattice key and Properties key. The typical output of a QUIP command can also be seen in Figure \ref{fig:quip} where package information is printed out first and then the energy that QUIP predicts is returned. We will be using the QUIP software as reference calculations to experimental results. 
The Carbon GAP 20 potential\cite{gap20} will be used as the parameters in the QUIP calculations.

\begin{figure}
  \centering
  \includegraphics[scale=.5]{quip.png}
  \includegraphics[scale=.5]{quip4.png}
  
  \caption{A) is the QUIP command line. B) is the example.xyz file used in the A) command line with the required Lattice and Properties keyd. C) is the output of the QUIP command with the predicted energy highlighted. }\label{fig:quip}
\end{figure}


\newpage


\subsection{Machine Learning}

% graph representation (molecules as graphs)
% types of problems that use graph structured data 
In a molecule, different pairs of atoms and bonds have different distances so it is easy to describe
these molecules as a graph. The atoms are the nodes and the bonds are the undirected edges. In general, 
there are three types of prediction tasks that are done on graphs: graph-level, node-level, and edge-level. 
Graph-level approaches predict a single property for a whole graph while a node-level approach predicts 
a property for each node in the graph. Edge-level approaches predict the property or existence of edges 
in a particular graph. We will be focusing on the graph-level task of predicting the energy of carbon 
molecules. 

% integrating graphs in neural networks (matrices, adjacency lists)
Integrating these graphs into machine learning models will depend on how they are represented. 
An obvious representation method would be the adjacency matrix since it captures the graph's connectivity. 
However, the more nodes there are, the more space-inefficient the adjacency matrix becomes and in addition,
the adjacency matrix of a molecule is not unique. There are many different ways to express the same 
connectivity with these matrices which causes the model to be permutation variant since it will see 
these matrices as different molecules when they are actually the same. A better memory-efficient representation 
is adjacency lists where each item in this list describes the connection between just two nodes. This cuts out 
a lot of the parts of the graph that are disconnected that would be needlessly recorded in an adjacency matrix.


\subsubsection{Graph Neural Networks}    

% Pooling and Message Passing
A graph neural network (GNN) consists of transformations on all attributes of the given graph 
that preserves its symmetries and thus is permutation invariant. The simplest kind of GNN has 
each layer apply a multilayer perceptron (MLP) on each attribute of a graph which includes the
nodes, edges, and global-context. Since predictions are made on the nodes of the graph, \emph{pooling} 
is a way to gather information about the edges and give them to the nodes. This is done by 
gathering the embeddings of each item that will be pooled into a matrix and then aggregate them, 
usually through a summation. More complex predictions can be made by using pooling 
in each of the GNN layers which lets information exchange influence updated embeddings. This 
allows the embeddings to be aware of the graph's connectivity and is called \emph{message passing}. 
The forward pass of a message passing neural network (MPNN) consists of two steps: message passing 
and readout. In message passing, each node has a hidden state or feature vector and a function of 
neighboring hidden states are aggregated for each node. The hidden state of a node is updated using 
the obtained message and the previous hidden state of that node. Conceptually this allows each node 
to understand and use previous history to influence their future update. The readout step computes 
a feature vector for the whole graph.

% Graph convolution
\begin{figure}
  \centering
  \includegraphics[scale=1.75]{mpnn.jpeg}
  
  \caption{Simple example of message passing in a graph. Image is author's own image.}\label{fig:mpnn}
\end{figure}

A specific way to carry out message passing is by using graph convolutions. Just like in convolutional 
neural networks, the input is multiplied by a set of weights known as \emph{kernels}. It acts like a 
filter that scans across the input graph and allows the model to learn features from neighboring nodes, as shown in figure \ref{fig:mpnn}.
Convolutions collect and condense information about local environments within a graph to learn about 
its connectivity. Through this kernel, neighboring information can be aggregated with the information 
the node is already holding and thus performs message passing throughout the convolutions.




\newpage 

\section{Method}
Our goal is to model the potential energy surface of carbon molecules through machine learning. The challenge is to have the model not be limited to the number of atoms each molecule has and that it is invariant to translation, rotation, and permutation. Our approach takes advantage of the fact that a molecule is made up of consistent structural units to create a network made up of all the local "neighborhoods" of the molecule. Our graph representation allows the energy of the whole molecule to be found through local environment information, which will be described in more detail in section \ref{Train}.

Several current research work\cite{ex1,ex2,gap20} of similar domains tend to use a Gaussian Approximation Potential (GAP) to model molecules. The GAP framework allows training with first-principles calculations at a significantly reduced cost with similar predictive quality. Even though these GAP frameworks are transferable and show good performance, its downside lies in its $O(N^3)$ computational cost, even though it is better than other previous methods of molecular simulations. In addition, not all kinds of configurations can be represented by GAP so data that does not display a Gaussian uncertainty can be difficult to predict. We propose a message passing graph neural network as a more cost effective technique to emulate what GAPs are doing to model the potential energy surface of carbon structures.

\subsubsection{Model} 
% Model
The model uses the python package DGL-LifeSci\cite{dgllife} which allows an easy integration of tasks in 
chemistry and graph neural networks. The implemented model is split into three parts: message passing, 
readout, and linear layers. The message passing graph neural network (MPNNGNN) from DGL-LifeSci first starts 
with a linear and ReLU layer. Then there is a cycle that consists of a graph convolution layer, another 
ReLU layer, and a multi-layer gated recurrent unit (GRU). This cycle is repeated for the number of 
messsage passing set in the beginning and through this repetition, node representations are being updated 
and message passing is being performed. The second part of the model is a MLP-based readout where it 
updates node representations with a MLP and then computes graph representations out of those node features. 
The third part of the model is a simple combination of linear, dropout, ReLU, linear layers.

% Permutation Invariance
An important property of this model is that it is permutation invariant. This means that the output of 
the network does not depend on the order of the input. To put it another way, say that there is a singular
water molecule with its atoms labeled "oxygen", "hydrogen 1", and "hydrogen 2". The two hydrogen atoms 
are not conceptually different in this molecule so it should not matter if "hydrogen 2" gets inputted 
before "hydrogen 1" in the model. This model is permutation invariant because nodes are communicating 
between each other and are learning from local graph information through message passing. Message passing 
allows the nodes not to be dependent on a particular input and its order.




\newpage

\section{Experimental Evaluation}
\subsubsection{Training Dataset}\label{Train}
%Carbon GAP 20 data
The data that is used comes from a database of configurations called Carbon Data Set Total\cite{gap20}. For the model to be accurate and transferable, the database represents a reduced subset of all available configurations from the thermally accessible chemical space. The molecular properties were computed using the same level of tightly converged plane-wave DFT including dispersion corrections. The VASP plane-wave DFT code was used to perform spin-polarized calculations with the optB88-vdW dispersion inclusive exchange–correlation functional, a plane-wave cutoff of 600 eV, and a projector augmented wave pseudopotential. In addition, dense reciprocal space Monkhorst-Pack grids were used, along with a Gaussian smearing of 0.1 eV, applied to the energy levels. Some \emph{ab initio} and iteratively improved GAP driven molecular dynamics simulations have been performed for all of these structures at a number of temperatures. 

This comprehensive database contains approximately 17,000 configurations, which is too large to conduct training at a reasonable computational cost. A subset of this database, called Carbon GAP 20 Training set, was chosen to use in training through the farthest point sampling method. After that, data saturation of this sampling was checked to ensure that this new training dataset was as comprehensive as the full database. The training data contains 6,088 configurations of mostly crystalline carbon and some liquid and amorphous carbon with their respective properties. As shown in Figure \ref{fig:train}, the full database is very comprehensive for all possible crystalline phases of carbon at moderate temperatures and pressures with the additional inclusion of more exotic allotropes. 


\begin{figure}
  \centering
  \includegraphics[scale=2]{carbon}
  
  \caption{Map of the training data with some of the key structures (top). Summary of predicted formation energies, defect formation energies, and surface energies of crystalline carbon (bottom). Image source from GAP20\cite{gap20}}\label{fig:train}
\end{figure}


% Graph dataset
This dataset of geometries is then turned into a graph dataset. For each atom in each molecule, four nearest neighbors are found and the distances are calculated to that atom. Once the the distances for the four nearest neighbors of each atom is found, it is used to create a DGL\cite{dgl} graph that represents the whole molecule where the edge features of the graph contain the distance information. This method of representation uses the local environment of each atom to describe a whole molecule and determine its energy. This representation is both rotation and translation invariant. This is because the distance between these atoms do not change if rotated or translated, thus it does not depend on its specific Cartesian location.



\subsubsection{Test Dataset} 
% DUNN and Modified Total
Two datasets are used to test the performance of the model. One dataset contains carbon allotropes of monolayer graphene, bilayer graphene, graphite, and diamond with their DFT energies calculated using VASP. For simplicity, this dataset will be called the DUNN dataset for it was originally used to develop a Dropout Uncertainty Neural Network potential\cite{dunn}. The second test set comes from the source of the training set, Carbon Data Set Total. Since the training data is a subset, it was removed from the total dataset to create a Modified Total dataset to test on. The graph dataset versions of these test sets are created the same way the training graph dataset was made, referenced in section \ref{Train}. 

% Dummy, Untrained, and QUIP
Three other measurements were made to additionally evaluate the model performance. The first measurement uses the dummy regressor from Scikit-learn to make predictions and calculate its mean squared error (MSE) on both the DUNN and Modified Total test set. The second measurement uses the untrained version of the model to directly calculate predictions and MSE for both test sets to see how the model performs without any learning. The third measurement uses the QUIP software and the GAP 20 potential\cite{gap20} to calculate energies and MSE on both test sets once again. This measurement will allow the direct comparison of the cited research's results with our results.

\subsubsection{Results} 

\begin{table}
    \centering
    \begin{tabular}{|p{3cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
    \hline
    \multicolumn{5}{|c|}{Parameters} \\
    \hline
    Run & Learning Rate & Message Passing & Batch Size & Nearest Neighbor\\
    \hline
    1 & 0.0005 & 5 & 16 & 4\\
    2 & 0.0005 & 7 & 32 & 4\\    
    3 & 0.0001 & 5 & 32 & 4\\
    Untrained (DUNN) & 0.0005 & 5 & & 4\\
    4 & 0.0005 & 5 & 16 & 4\\
    5 & 0.0005 & 7 & 32 & 4\\
    6 & 0.0001 & 5 & 32 & 4\\
    Untrained (Modified Total) & 0.0005 & 5 & & 4\\
    \hline
    
    \end{tabular}
    \caption{Table of the differences in parameters between the top six runs}\label{tab:param}
\end{table}

Experiments were started with 1 and 3 rounds of message passing with a batch size of 16, learning rate of 0.001, and 4 nearest neighbors. For those runs, the model was overfitting the training data because the train loss was gradually decrease while the test loss was unstable. We then just increased the number of message passing to 5 but those as well had unstable test losses even when the train losses were very stable. Because of this, we kept the number of message passing at 5 but lowered the learning rate and varied the batch size. Once both train and test losses seemed to gradually decrease, we also ran some experiments with the message passing set to 7. The specifications of the experiments done with message passing of 5 and 7 can be seen in Table \ref{tab:param} and the graphs of their MSE can be seen in Figure \ref{fig:graph}.

First, the results clearly show that the model is able to train. All the experiments produce an MSE lower than the untrained model for both test sets which shows that the model is working. All the runs also have an MSE lower than the dummy regressor baseline which proves it is learning better than a predictor with a simple strategy. The model is definitely able to learn the potential energy surface because its train and test losses are consistently improving and becoming more stable as the number of message passing increases. The runs with the same number of message passing also improve compared to each other as other parameters are adjusted because the MSE is lowering at several magnitudes between each other as seen in Table \ref{tab:dunn} and Table \ref{tab:total}.

Even though our test datasets come from different sources, the model seems to improve and learn those datasets in a similar way which can be seen by comparing Table \ref{tab:dunn}, Table \ref{tab:total}, and Table \ref{tab:param}. This demonstrates the transferable, polymorphic, and physically-plausible properties of the model. However, the MSE of the QUIP software is still considerably lower than any of the experiments with the MPGNN model which shows that there is still room for improvement to the model. The decreasing losses of the MPGNN model experiments show the possibility for further improvement and better future performance through even more fine tuning of the model. Since the MPGNN model was able to learn the potential energy surface of carbon molecules, it can still be proposed as a much less costly approach to polymophic modelling compared to current GAP frameworks. 


% graph
\begin{figure}
  \centering
  \includegraphics[scale=.5]{test.png}
  \includegraphics[scale=.5]{best.png}
  
  \caption{Both legends refer to the runs in Table \ref{tab:param}. Upper graph shows the MSE test losses for each of the six runs. Lower graph shows the best MSE losses for each of the six runs. }\label{fig:graph}
\end{figure}

\begin{table}
    \centering
    \begin{tabular}{p{3cm}|p{3cm}}
    \hline
    \multicolumn{2}{|c|}{DUNN Dataset} \\
    \hline
    Run  & MSE (ev)\\
    \hline
    QUIP   & 45.256\\
    1   & 1,655.831\\
    2    & 2,516.364\\
    3 & 18,836.808\\
    Dummy & 25,620.847\\
    Untrained & 228,335.155\\
    \hline
    \end{tabular}
    \caption{Table of the MSE of the runs done with the DUNN dataset.}\label{tab:dunn}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{p{3cm}|p{3cm}}
    \hline
    \multicolumn{2}{|c|}{Modified Total Dataset} \\
    \hline
    Run  & MSE (ev)\\
    \hline
    QUIP   & 16.989 \\
    4   & 3,388.47 \\
    5 & 7,583.152 \\
    6    & 8,438.149\\
    Dummy & 191,405.849 \\
    Untrained & 418,529.850\\
    \hline
    \end{tabular}
    \caption{Table of the MSE of the runs done with the Modified Total dataset.}\label{tab:total}
\end{table}



\subsubsection{Suggestions} 

Our findings open up the potential for more improvement in future work and experimentation. Adjustments to hyperparameters, trying different activation functions in the model, and using even higher numbers of message passing would be ways to further continue the study of this MPGNN model. Another suggestion could be to change the graph representation to a hypergraph representation where an edge can join any number of vertices unlike in a traditional graph where each edge connects exactly two vertices. If the ternary edges contained angle information between atoms, the model would have even more information, distances and angles of nearest neighbors, to learn the potential energy surface. The message passing portion of the model would be able to learn about surrounding nodes even better with the additional information they hold. The many possibilities for future research demonstrate the potential capability of the MPGNN model and further validate it as an inexpensive viable approach to modelling carbon structures.


\newpage

\bibliographystyle{plain}
\bibliography{ref}


\end{document}
